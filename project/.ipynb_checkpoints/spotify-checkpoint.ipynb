{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['is','a','at','an','the','of','it','and','as']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalize playlist names by removing some stopwords.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    name : str\n",
    "        Playlist name to normalize\n",
    "    \"\"\"\n",
    "    global stopwords\n",
    "    name = name.lower()\n",
    "    querywords = name.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "    name = ' '.join(resultwords)\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "def get_mapping(uri_list,uri2name):\n",
    "    \"\"\"\n",
    "    Create a dictionary to obtain:\n",
    "    1. ID from URI \n",
    "    2. Name from ID\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    uri_list : list\n",
    "        List of unique URI's.\n",
    "    uri2name : dict\n",
    "        Key value mapping between URI and Name.\n",
    "    \"\"\"\n",
    "    uri2id = dict()\n",
    "    id2name = []\n",
    "    for index,item in enumerate(uri_list):\n",
    "        uri2id[item] = index\n",
    "        id2name.append(uri2name[item])\n",
    "    return uri2id,id2name\n",
    "\n",
    "def create_preprocess_info(num_files,dataset):\n",
    "    \"\"\"\n",
    "    Obtain some preprocessing information such \n",
    "    as track_ids, artist_ids, etc. from the entire MPD data.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    num_files : int\n",
    "        Number of files to read from MPD.\n",
    "    \"\"\"\n",
    "    p_names = []\n",
    "    #track_uri,artist_uri = (set() for i in range(2))\n",
    "    track_uri2name,artist_uri2name,track_uri2artist_uri = (dict() for i in range(3))\n",
    "    files = (os.listdir(\"../mpd/data\"))\n",
    "    ordered_files = sorted(files, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "    num_occurences_track, num_occurences_artist = {},{}\n",
    "    for filename in tqdm(ordered_files[:num_files],ncols=100):\n",
    "        filename = os.path.join('../mpd/data',filename)\n",
    "        with open(filename, 'r') as fp:\n",
    "            playlists = json.load(fp)['playlists']\n",
    "        for p in playlists:\n",
    "            p_names.append(normalize_name(p['name']))\n",
    "            for song in p['tracks']:\n",
    "                if song['track_uri'] in num_occurences_track:\n",
    "                    num_occurences_track[song['track_uri']] += 1\n",
    "                    num_occurences_artist[song['artist_uri']] += 1\n",
    "                else:\n",
    "                    num_occurences_track[song['track_uri']] = 1\n",
    "                    num_occurences_artist[song['artist_uri']] = num_occurences_artist.get(song['artist_uri'],0) + 1\n",
    "                    track_uri2name[song['track_uri']] = song['track_name']\n",
    "                    artist_uri2name[song['artist_uri']] = song['artist_name']\n",
    "                    track_uri2artist_uri[song['track_uri']] = song['artist_uri']\n",
    "    \n",
    "    track_uri = [x for x in num_occurences_track.keys() if num_occurences_track[x]>=1]\n",
    "    artist_uri = [x for x in num_occurences_artist.keys() if num_occurences_artist[x]>=1]\n",
    "    \n",
    "    remove_tracks = [x for x in num_occurences_track.keys() if num_occurences_track[x]<1]\n",
    "    remove_artists = [x for x in num_occurences_artist.keys() if num_occurences_artist[x]<1]\n",
    "    \n",
    "    for t in remove_tracks:\n",
    "        del track_uri2name[t]\n",
    "        del track_uri2artist_uri[t]\n",
    "    \n",
    "    for a in remove_artists:\n",
    "        del artist_uri2name[a]\n",
    "    \n",
    "    with open('../mpd/preprocessed/playlist_names.pkl','wb') as fp:\n",
    "        pickle.dump(p_names,fp)                \n",
    "    \n",
    "    track_uri2id,track_id2name = get_mapping(track_uri,track_uri2name)\n",
    "    \n",
    "    with open('../mpd/preprocessed/tracks-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump([track_uri2id,track_id2name],fp)\n",
    "    \n",
    "    #album_uri2id,album_id2name = get_mapping(album_uri,album_uri2name)\n",
    "        \n",
    "    #with open('../mpd/preprocessed/albums.pkl','wb') as fp:\n",
    "    #    pickle.dump([album_uri2id,album_id2name],fp)\n",
    "\n",
    "    artist_uri2id,artist_id2name = get_mapping(artist_uri,artist_uri2name) \n",
    "    #Removing a problem of same artist having multiple uri's.\n",
    "    repeating_artists = {}\n",
    "    for k, v in artist_uri2name.items():\n",
    "        repeating_artists.setdefault(v, []).append(k)\n",
    "    repeating_artists = { k:v for k, v in repeating_artists.items() if len(v)>1 }\n",
    "    for k,v in repeating_artists.items():\n",
    "        index = artist_uri2id[v[0]]\n",
    "        for uri in v[1:]:\n",
    "            artist_uri2id[uri] = index\n",
    "    \n",
    "    with open('../mpd/preprocessed/artists-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump([artist_uri2id,artist_id2name],fp)\n",
    "        \n",
    "    #Track ID to Artist ID\n",
    "    track_id2artist_id = dict()\n",
    "    for k,v in track_uri2artist_uri.items():\n",
    "        track_id2artist_id[track_uri2id[k]] = artist_uri2id[v]\n",
    "    \n",
    "    with open('../mpd/preprocessed/track2artist-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump(track_id2artist_id,fp)\n",
    "    \n",
    "    return [track_uri2id,track_id2name,artist_uri2id,artist_id2name,track_id2artist_id]\n",
    "\n",
    "def generate_data(num_files,dataset):\n",
    "    \"\"\"\n",
    "    Create a Pandas DataFrame for the entire dataset.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    num_files : int\n",
    "        Number of files to read from MPD.\n",
    "    dataset : str\n",
    "        Name of the dataset to be used while storing.\n",
    "    \"\"\"\n",
    "    files = (os.listdir(\"../mpd/data\"))\n",
    "    ordered_files = sorted(files, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "    data = [list() for x in range(num_files*1000)]\n",
    "    count = 0\n",
    "    index = 0\n",
    "    for filename in tqdm(ordered_files[:num_files],ncols=100):\n",
    "        filename = os.path.join('../mpd/data',filename)\n",
    "        with open(filename, 'r') as fp:\n",
    "            playlists = json.load(fp)['playlists']\n",
    "        for p in playlists:\n",
    "            for song in p['tracks']:\n",
    "                if song['track_uri'] in track_uri2id and song['artist_uri'] in artist_uri2id:\n",
    "                    data[index].append(str(track_uri2id[song['track_uri']]))\n",
    "                else:\n",
    "                    count+=1\n",
    "                    continue\n",
    "            index+=1\n",
    "    print('Omitted songs =',count,'\\nTotal Playlists =',len(data))\n",
    "    return data\n",
    "\n",
    "def train_test_split(ratings, split_count, fraction=None):\n",
    "    \"\"\"\n",
    "    Split recommendation data into train and test sets\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    data : list of lists\n",
    "        List of users containing lists of items they like.\n",
    "    split_count : int\n",
    "        Number of user-item-interactions per user to move\n",
    "        from training to test set.\n",
    "    fractions : float\n",
    "        Fraction of users to split off some of their\n",
    "        interactions into test set. If None, then all \n",
    "        users are considered.\n",
    "    \"\"\"\n",
    "    possible_users = [x for x in range(len(ratings)) if len(ratings[x])>=2*split_count]\n",
    "    test_list = np.random.choice(possible_users, replace=False,\n",
    "                                 size = np.int32(np.floor(fraction*len(ratings))))\n",
    "    test = []\n",
    "    for index in test_list:\n",
    "        to_add = ratings[index][-split_count:]\n",
    "        ratings[index] = ratings[index][:-split_count]\n",
    "        test.append(copy.deepcopy(to_add))\n",
    "    train = ratings\n",
    "    return train,test,test_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommendations(model,ratings,pid_list,k,update=False):\n",
    "    \"\"\"\n",
    "    Get recommendations for playlists from the \n",
    "    trained recommendation model.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : object\n",
    "        Trained recommendation model.\n",
    "    ratings : scipy.sparse matrix\n",
    "        Interactions between users and items.\n",
    "    pid_list : list\n",
    "        List of playlist ID's to recommend\n",
    "    \"\"\"\n",
    "    recommended_tracks = []\n",
    "    for pid in tqdm(pid_list):\n",
    "        for rec_track, score in model.recommend(pid,ratings,N=k,recalculate_user=update) :\n",
    "            recommended_tracks.append([pid,rec_track,score])\n",
    "    return sorted(recommended_tracks,key=lambda x: (x[0],-x[2]))\n",
    "\n",
    "def recommend(model,ratings,k,update=True):\n",
    "    \"\"\"\n",
    "    Get recommendations for a particular user.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : model\n",
    "        Trained recommendation model.\n",
    "    ratings : scipy.sparse matrix\n",
    "        Interactions between user and items. [1xN]\n",
    "    k : int\n",
    "        K for calculating precision@K.\n",
    "    \"\"\"\n",
    "    recommendations = sorted(model.recommend(0,ratings,N=k,recalculate_user=update),key=lambda x: (-x[1]))\n",
    "    tracks = []\n",
    "    for rec in recommendations:\n",
    "        tracks.append((track_id2name[rec[0]],track_id2artist_id[rec[0]]),rec[1])     \n",
    "    return tracks\n",
    "\n",
    "def save_model(model,filename):\n",
    "    \"\"\"\n",
    "    Save the trained recommendation model.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : object\n",
    "        Trained recommendation model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('../mpd/models/'+filename+'.pkl','wb') as fp:\n",
    "            pickle.dump(model,fp)\n",
    "    except:\n",
    "        return None\n",
    "    return 1\n",
    "\n",
    "def precision_at_k(expected, predicted):\n",
    "    \"\"\"\n",
    "    Compute precision@k metric. Also known as hit-rate.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predicted : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for i in range(len(expected)):\n",
    "        precision = float(len(set(predicted[i]) & set(expected[i]))) / float(len(predicted[i]))\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions) \n",
    "\n",
    "def compute_dcg(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute DCG score for each user.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list\n",
    "        Ground truth recommendations for single playlist.\n",
    "    predicted : list\n",
    "        Predicted recommendations for single playlist.\n",
    "    \"\"\"\n",
    "    score = [float(el in expected) for el in predicted]\n",
    "    dcg = np.sum(score / np.log2(1 + np.arange(1, len(score) + 1)))\n",
    "    return dcg\n",
    "\n",
    "def dcg_at_k(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute dcg@k metric. (Discounted Continuous Gain)\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predictions : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    dcg_scores = []\n",
    "    for i in range(len(expected)):\n",
    "        dcg = compute_dcg(expected[i],predicted[i])\n",
    "        dcg_scores.append(dcg)\n",
    "    return np.mean(dcg_scores)\n",
    "\n",
    "def ndcg_at_k(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute ndcg@k metric. (Normalized Discounted Continous Gain)\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predicted : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    for i in range(len(expected)):\n",
    "        labels = expected[i]\n",
    "        idcg = compute_dcg(labels,labels)\n",
    "        true_dcg = compute_dcg(labels,predicted[i])\n",
    "        ndcg_scores.append(true_dcg/idcg)\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def compute_metrics(ratings,predictions,pid_list,k):\n",
    "    \"\"\"\n",
    "    Wrapper function to compute all metrics.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    ratings : scipy.sparse matrix\n",
    "        Interactions between users and items.\n",
    "    predictions : pd.DataFrame\n",
    "        DataFrame containing all predictions <pid,Recommended,Score>\n",
    "    pid_list : list\n",
    "        List of playlist ID's in predictions\n",
    "    k : int\n",
    "        K for calculating metric@K.\n",
    "    \"\"\" \n",
    "    predicted = []\n",
    "    expected = []\n",
    "    for pid in tqdm(pid_list):\n",
    "        top_k = predictions.loc[predictions['pid']==pid][['Recommended','Score']]\\\n",
    "                    .sort_values('Score',ascending=False)['Recommended'].tolist()\n",
    "        labels = ratings.getrow(pid).indices\n",
    "        predicted.append(top_k)\n",
    "        expected.append(labels)\n",
    "    \n",
    "    patk = precision_at_k(expected,predicted)\n",
    "    dcgatk = dcg_at_k(expected,predicted)\n",
    "    ndcgatk = ndcg_at_k(expected,predicted)\n",
    "    return [patk,dcgatk,ndcgatk]\n",
    "\n",
    "def new_playlist(model,songs):\n",
    "    track_ids = [track_uri2id[s] for s in songs if s in track_uri2id]\n",
    "    return track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for future reference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.70s/it]\n"
     ]
    }
   ],
   "source": [
    "num_files = 1\n",
    "dataset = '{0}k'.format(num_files)\n",
    "importance = 1\n",
    "k = 1\n",
    "\n",
    "if(1):\n",
    "    print('Preprocessing data for future reference')\n",
    "    track_uri2id,track_id2name,\\\n",
    "    artist_uri2id,artist_id2name,\\\n",
    "    track_id2artist_id = create_preprocess_info(num_files,dataset)\n",
    "else:\n",
    "    with open('../mpd/preprocessed/tracks-'+dataset+'.pkl','rb') as fp:\n",
    "        track_uri2id,track_id2name = pickle.load(fp)\n",
    "\n",
    "    with open('../mpd/preprocessed/artists-'+dataset+'.pkl','rb') as fp:\n",
    "        artist_uri2id,artist_id2name = pickle.load(fp)\n",
    "\n",
    "    with open('../mpd/preprocessed/track2artist-'+dataset+'.pkl','rb') as fp:\n",
    "        track_id2artist_id = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training and Testing set for 1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitted songs = 0 \n",
      "Total Playlists = 1000\n",
      "Saving Training and Testing Set...\n"
     ]
    }
   ],
   "source": [
    "if(1):\n",
    "    print('Generating Training and Testing set for',dataset)\n",
    "    data = generate_data(num_files,dataset)\n",
    "    with open('../mpd/neural/data-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump(data,fp)      \n",
    "    train,test,pid_list = train_test_split(data,10,fraction=0.2)\n",
    "    print('Saving Training and Testing Set...')\n",
    "    with open('../mpd/neural/train-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump(train,fp)   \n",
    "    with open('../mpd/neural/test-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump(test,fp)  \n",
    "    np.save('../mpd/neural/pid_list-'+dataset+'.npy',pid_list)\n",
    "else:\n",
    "    with open('../mpd/neural/data-'+dataset+'.pkl','rb') as fp:\n",
    "        data = pickle.load(fp) \n",
    "    with open('../mpd/neural/train-'+dataset+'.pkl','rb') as fp:\n",
    "        train = pickle.load(fp)\n",
    "    with open('../mpd/neural/test-'+dataset+'.pkl','rb') as fp:\n",
    "        test = pickle.load(fp)\n",
    "    pid_list = np.load('../mpd/neural/pid_list-'+dataset+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34443"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(track_uri2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saboo.Varun\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\saboo.Varun\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Playlists = 1000\n",
      "Number of Tracks = 34443\n"
     ]
    }
   ],
   "source": [
    "print('Number of Playlists =',len(data))\n",
    "print('Number of Tracks =',len(track_uri2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(data,embedding_dim,precomputed=False):\n",
    "    if(not precomputed):\n",
    "        print('Creating Word2Vec Model...')\n",
    "        model = Word2Vec(data, size=embedding_dim, window=5, min_count=1, workers=6, sg=1)\n",
    "        model.save('../mpd/preprocessed/word2vec-'+str(embedding_dim)+'.txt')\n",
    "    else:\n",
    "        model = Word2Vec.load('../mpd/preprocessed/word2vec-'+str(embedding_dim)+'.txt')\n",
    "\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), embedding_dim))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return model,embedding_matrix\n",
    "\n",
    "def getBatch(data,w2v_model,batch_index,BATCH_SIZE=256):\n",
    "    start_index = batch_index*BATCH_SIZE\n",
    "    end_index = min((batch_index+1)*BATCH_SIZE,len(data))\n",
    "    inputs = data[start_index:end_index]\n",
    "    user_input,item_input,expected_score = map(lambda x: np.asarray(x).reshape((BATCH_SIZE,1)),zip(*inputs))\n",
    "    w2v_input = [w2v_model.wv.vocab[str(x)].index for x in np.nditer(item_input)]\n",
    "    w2v_input = np.asarray(w2v_input).reshape((BATCH_SIZE,1))\n",
    "    \n",
    "    return [user_input,\n",
    "            item_input,\n",
    "            w2v_input,\n",
    "            expected_score\n",
    "           ]\n",
    "    \n",
    "def reformatData(data):\n",
    "    positive_samples = []\n",
    "    for index,x in enumerate(tqdm(data)):\n",
    "        for ele in x:\n",
    "            positive_samples.append([index,ele,1])\n",
    "    return positive_samples\n",
    "\n",
    "def addNegativeSampling(dataset,num_negative = 100):\n",
    "    all_tracks = np.arange(num_tracks)\n",
    "    negative_samples = []\n",
    "    for index,user_ratings in enumerate(tqdm(dataset)):\n",
    "        possible_negs = np.setdiff1d(all_tracks,user_ratings,assume_unique=True)\n",
    "        negative_tracks = np.random.choice(possible_negs,replace=False,size=num_negative).tolist()\n",
    "        for ele in negative_tracks:\n",
    "            negative_samples.append([index,ele,0])\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "num_users = len(data)\n",
    "num_tracks = len(track_uri2id)\n",
    "hidden_layers = [512,256,128]\n",
    "num_epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:48<00:00, 20.69it/s]\n"
     ]
    }
   ],
   "source": [
    "if(1):\n",
    "    w2v_model,embedding_matrix = getEmbeddingMatrix(data,embedding_dim,precomputed=False)\n",
    "    positive_samples = reformatData(train)\n",
    "    negative_samples = addNegativeSampling(data)\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    with open('../mpd/neural/all_samples-'+dataset+'.pkl','wb') as fp:\n",
    "        pickle.dump(all_samples,fp)\n",
    "else:\n",
    "    w2v_model,embedding_matrix = getEmbeddingMatrix(data,embedding_dim,precomputed=True)\n",
    "    with open('../mpd/neural/all_samples-'+dataset+'.pkl','rb') as fp:\n",
    "        all_samples = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\t"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'7335'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-eae4fcd022a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedded_items\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             _,op,batch_loss = sess.run([optimizer,output,loss],feed_dict={user_input:users,\n\u001b[0;32m     78\u001b[0m                                                                   \u001b[0mitem_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-3c42a7567e6e>\u001b[0m in \u001b[0;36mgetBatch\u001b[1;34m(data, w2v_model, batch_index, BATCH_SIZE)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0muser_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpected_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mw2v_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mw2v_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-3c42a7567e6e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0muser_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpected_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mw2v_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mw2v_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '7335'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/gpu:0'):\n",
    "    user_input = tf.placeholder(dtype=tf.int32, shape=(None,1), name='user_input')\n",
    "    item_input = tf.placeholder(dtype=tf.int32, shape=(None,1), name='item_input')\n",
    "    w2v_input = tf.placeholder(dtype=tf.int32, shape=(None,1), name='w2v_input')\n",
    "    expected_score = tf.placeholder(dtype=tf.float32, shape=(None,1), name='expected_score')\n",
    "    with tf.name_scope('embedding'):\n",
    "        #Item embedding for MLP\n",
    "        W_item_mlp = tf.Variable(tf.constant(0.0, shape=[num_tracks, embedding_dim]),\n",
    "                        trainable=False, name='W_item_mlp')\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [num_tracks, embedding_dim],name='embedding_placeholder')\n",
    "        embedding_init = W_item_mlp.assign(embedding_placeholder)\n",
    "        item_mlp_embedding = tf.nn.embedding_lookup(W_item_mlp, w2v_input)\n",
    "        \n",
    "        #W_item_mlp = tf.Variable(tf.random_normal([num_tracks, embedding_dim], 0, 0.1),\n",
    "        #                name='W_item_mlp')\n",
    "        #item_mlp_embedding = tf.nn.embedding_lookup(W_item_mlp, item_input)\n",
    "        \n",
    "        #User Embedding for MLP\n",
    "        W_user_mlp = tf.Variable(tf.random_normal([num_users, embedding_dim], 0, 0.1),\n",
    "                        name='W_user_mlp')\n",
    "        user_mlp_embedding = tf.nn.embedding_lookup(W_user_mlp, user_input)\n",
    "\n",
    "        #Item Embedding for MF\n",
    "        W_item_mf = tf.Variable(tf.random_normal([num_tracks, embedding_dim], 0, 0.1),\n",
    "                        name='W_item_mf')\n",
    "        item_mf_embedding = tf.nn.embedding_lookup(W_item_mf, item_input)\n",
    "\n",
    "        #User Embedding for MF\n",
    "        W_user_mf = tf.Variable(tf.random_normal([num_users, embedding_dim], 0, 0.1),\n",
    "                        name='W_user_mf')\n",
    "        user_mf_embedding = tf.nn.embedding_lookup(W_user_mf, user_input)\n",
    "    \n",
    "    with tf.name_scope('post_embedding'):\n",
    "        mf_output = tf.multiply(tf.layers.Flatten()(user_mf_embedding),\n",
    "                                   tf.layers.Flatten()(item_mf_embedding))\n",
    "\n",
    "        mlp_latent_vec = tf.concat([tf.layers.Flatten()(user_mlp_embedding),\n",
    "                                    tf.layers.Flatten()(item_mlp_embedding)],\n",
    "                                    axis=1)\n",
    "    layers = {}\n",
    "    layers_compute = {}\n",
    "    \n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        for i in range(1,len(hidden_layers)+1):   \n",
    "            if(i==1):\n",
    "                l = tf.layers.dense(inputs=mlp_latent_vec,\n",
    "                                    units=hidden_layers[i-1],\n",
    "                                    activation=tf.nn.relu)\n",
    "            else:\n",
    "                l = tf.layers.dense(inputs=layers_compute[i-2],\n",
    "                                    units=hidden_layers[i-1],\n",
    "                                    activation=tf.nn.relu)\n",
    "            layers_compute[i-1] = l\n",
    "    mlp_output = layers_compute[i-1]\n",
    "    predict_layer_input = tf.concat([tf.layers.Flatten()(mf_output),\n",
    "                                   tf.layers.Flatten()(mlp_output)],\n",
    "                                  axis=1)\n",
    "    output = tf.layers.dense(inputs=predict_layer_input,\n",
    "                             units=1,\n",
    "                             activation=tf.nn.sigmoid)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=expected_score,logits=output))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_list = []\n",
    "    for e in range(num_epochs):\n",
    "        print('EPOCH:',e+1,end='\\t')\n",
    "        start = datetime.now()\n",
    "        np.random.shuffle(all_samples)\n",
    "        num_batches = np.int32(np.ceil(len(all_samples)//batch_size))\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            users,items,embedded_items,targets = getBatch(all_samples,w2v_model,i,batch_size)\n",
    "            _,op,batch_loss = sess.run([optimizer,output,loss],feed_dict={user_input:users,\n",
    "                                                                  item_input:items,\n",
    "                                                                  w2v_input:embedded_items,\n",
    "                                                                  expected_score:targets,\n",
    "                                                                  embedding_placeholder:embedding_matrix})\n",
    "            epoch_loss += batch_loss\n",
    "        epoch_loss /= num_batches\n",
    "        end = datetime.now()\n",
    "        epoch_time = end-start\n",
    "        print('LOSS:',epoch_loss,'\\tTime:',epoch_time.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'7335'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8bd17f9c1212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'7335'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrack_id2name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7335\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '7335'"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.vocab['7335'])\n",
    "track_id2name[7335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33702\n",
      "14355\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_model.wv.vocab))\n",
    "print(w2v_model.wv.index2word[7335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout\n",
    "from keras.layers import Multiply, Concatenate\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_ncf_model(num_user, num_item, latent_v_dim=8, \n",
    "                dense_layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0\n",
    "                ):\n",
    "\n",
    "    # Input layer\n",
    "    input_user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    input_item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    mf_user_embedding = Embedding(input_dim=num_user, output_dim=latent_v_dim,\n",
    "                        name='mf_user_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mf_item_embedding = Embedding(input_dim=num_item, output_dim=latent_v_dim,\n",
    "                        name='mf_item_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mlp_user_embedding = Embedding(input_dim=num_user, output_dim=dense_layers[0]//2,\n",
    "                         name='mlp_user_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "    mlp_item_embedding = Embedding(input_dim=num_item, output_dim=dense_layers[0]//2,\n",
    "                         name='mlp_item_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "\n",
    "    # Matrix Factorization latent vector\n",
    "    mf_user_latent = Flatten()(mf_user_embedding(input_user))\n",
    "    mf_item_latent = Flatten()(mf_item_embedding(input_item))\n",
    "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # Multi layer perceptron latent vector\n",
    "    mlp_user_latent = Flatten()(mlp_user_embedding(input_user))\n",
    "    mlp_item_latent = Flatten()(mlp_item_embedding(input_item))\n",
    "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    \n",
    "    mlp_vector = mlp_cat_latent\n",
    "    # Build dense layer for model\n",
    "    for i in range(1,len(dense_layers)):\n",
    "        layer = Dense(dense_layers[i],\n",
    "                      activity_regularizer=l2(reg_layers[i]),\n",
    "                      activation='relu',\n",
    "                      name='layer%d' % i)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "    result = Dense(1, activation='sigmoid', \n",
    "                   kernel_initializer='lecun_uniform',name='result')\n",
    "\n",
    "    model = Model(input=[input_user,input_item], output=result(predict_layer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 61\n",
    "batch_size = 256\n",
    "latent_v_dim = 8\n",
    "dense_layers = [64, 32, 16, 8]\n",
    "reg_layers = [0, 0, 0, 0]\n",
    "reg_mf = [0]\n",
    "num_neg_sample = 4\n",
    "learning_rate = 0.001\n",
    "learner = 'adam'\n",
    "verbose = 1\n",
    "num_user = len(data)\n",
    "num_item = len(track_uri2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saboo.Varun\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
     ]
    }
   ],
   "source": [
    "model = build_ncf_model(num_user, num_item, latent_v_dim, dense_layers,\n",
    "        reg_layers, reg_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
