{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import msgpack\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import empty_playlist\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord2Vec(data,embedding_dim,min_count=5,path='',precomputed=False):\n",
    "    directory = path\n",
    "    w2v_fname = directory + '/word2vec-' + str(embedding_dim) + '.txt'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    if(not precomputed):\n",
    "        print('Creating Word2Vec Model...')\n",
    "        model = Word2Vec(data,size=embedding_dim,window=5,min_count=1,workers=6,iter=10,negative=10)\n",
    "        model.init_sims(replace=True)\n",
    "        print('Initialized Sims')\n",
    "        model.save(w2v_fname)\n",
    "        print('Saved Word2Vec model')\n",
    "    else:\n",
    "        print('Loading Word2Vec Model...')\n",
    "        model = Word2Vec.load(w2v_fname)\n",
    "        print('Loaded Word2Vec Model...')\n",
    "    return model\n",
    "\n",
    "def createAnnoyIndex(data,num_trees,path='.'):\n",
    "    dim = data[0].shape[0]\n",
    "    annoy_fname = path + '/annoy-'+str(num_trees)+'.ann'\n",
    "    annoy_indexer = AnnoyIndex(dim,metric='angular')  # Length of item vector that will be indexed\n",
    "    for key,value in tqdm_notebook(data.items()):\n",
    "        annoy_indexer.add_item(key, value)\n",
    "    print('Building Annoy Index...')\n",
    "    annoy_indexer.build(num_trees)\n",
    "    print('Saving Annoy Index...')\n",
    "    annoy_indexer.save(annoy_fname)\n",
    "    print('Done!')\n",
    "    return annoy_indexer\n",
    "    \n",
    "def loadAnnoyIndex(dim,num_trees,path=''):\n",
    "    annoy_fname = path + '/annoy-'+str(num_trees)+'.ann'\n",
    "    print(annoy_fname)\n",
    "    annoy_indexer = AnnoyIndex(dim,metric='angular')\n",
    "    annoy_indexer.load(annoy_fname) # super fast, will just mmap the file\n",
    "    #print(t.get_nns_by_item(0, 5)) # will find the 1000 nearest neighbors\n",
    "    return annoy_indexer\n",
    "\n",
    "def averagePlaylistVec(data):\n",
    "    avg_vec = np.zeros_like(w2v_model.wv.vectors[0])\n",
    "    playlist_vectors = {}\n",
    "    for index,playlist in enumerate(tqdm_notebook(data)):\n",
    "        w2vecs = []\n",
    "        for songid in playlist:\n",
    "            vec = w2v_model.wv[songid]\n",
    "            w2vecs.append(vec)\n",
    "        if(len(w2vecs)==0):\n",
    "            w2vecs.append(np.random.rand(w2v_model.wv.vectors[0].shape[0]))\n",
    "        avg_vec = np.mean(np.asarray(w2vecs),axis=0)\n",
    "        playlist_vectors[index] = avg_vec / np.linalg.norm(avg_vec)\n",
    "    return playlist_vectors\n",
    "\n",
    "def averageNewPlaylistVec(playlist):\n",
    "    avg_vec = np.zeros_like(w2v_model.wv.vectors[0])\n",
    "    w2vecs = []\n",
    "    for songuri in playlist:\n",
    "        songid = str(track_uri2id[songuri])\n",
    "        if(songid not in w2v_model.wv.vocab):\n",
    "            continue\n",
    "        vec = w2v_model.wv[songid]\n",
    "        w2vecs.append(vec)\n",
    "    avg_vec = np.mean(np.asarray(w2vecs),axis=0)\n",
    "    return avg_vec\n",
    "\n",
    "def testPlaylistAvg(data):\n",
    "    all_avg_vectors = [list() for i in range(len(data))]\n",
    "    names_01 = names[0:1000] + names[9000:]  \n",
    "    #similar_names = empty_playlist.get_topN_playlists(names_01, 10, 8)\n",
    "    #with open('similar_names_test.pkl','wb') as fp:\n",
    "    #    pickle.dump(similar_names,fp)\n",
    "    #print(similar_names)\n",
    "    with open('similar_names_test.pkl','rb') as fp:\n",
    "        similar_names = pickle.load(fp)\n",
    "    avg_vec = np.zeros_like(w2v_model.wv.vectors[0])\n",
    "    for key,value in tqdm_notebook(similar_names.items()):\n",
    "        w2vecs = []\n",
    "        for v in value:\n",
    "            if(v in playlist_vectors):\n",
    "                w2vecs.append(playlist_vectors[v])\n",
    "        avg_vec = np.mean(np.asarray(w2vecs),axis=0)\n",
    "        if(key<1000):\n",
    "            all_avg_vectors[key] = avg_vec\n",
    "        else:\n",
    "            if(len(data[8000+key]) != 0):\n",
    "                songid = data[8000+key][0]\n",
    "                vec_s = w2v_model.wv[songid]\n",
    "                vec_a = artist_vecs[track2artist[int(songid)]]\n",
    "                all_avg_vectors[8000+key] = 0.4*avg_vec + 0.4*vec_s + 0.2*vec_a\n",
    "            else:\n",
    "                all_avg_vectors[8000+key] = avg_vec\n",
    "    for index,playlist in enumerate(tqdm_notebook(data[1000:9000])):\n",
    "        w2vecs = []\n",
    "        for songid in playlist:\n",
    "            vec = w2v_model.wv[songid]\n",
    "            w2vecs.append(vec)\n",
    "        if(len(w2vecs)==0):\n",
    "            pl_name = names[1000+index]\n",
    "            if(pl_name!=''):\n",
    "                similar_names = empty_playlist.get_topN_playlists([pl_name], 10, 1)\n",
    "                for v in similar_names[0]:\n",
    "                    w2vecs.append(playlist_vectors[v])\n",
    "            else:\n",
    "                print('No name at index',1000+index)    \n",
    "        if(len(w2vecs) == 1):\n",
    "            new_w2vec = []\n",
    "            pl_name = names[1000+index]\n",
    "            if(pl_name!=''):\n",
    "                similar_names = empty_playlist.get_topN_playlists([pl_name], 10, 1)\n",
    "                for v in similar_names[0]:\n",
    "                    new_w2vec.append(playlist_vectors[v])\n",
    "                w2vecs.append(np.mean(np.asarray(new_w2vec),axis=0))\n",
    "            else:\n",
    "                print('No name at index',1000+index)\n",
    "                print('Getting artist match...')\n",
    "                w2vecs.append(artist_vecs[track2artist[int(playlist[0])]])\n",
    "                if(len(w2vecs) == 0):\n",
    "                    w2vecs.append(np.random.rand(w2v_model.wv.vectors[0].shape[0]))\n",
    "        avg_vec = np.mean(np.asarray(w2vecs),axis=0)\n",
    "        all_avg_vectors[1000+index] = avg_vec\n",
    "    return all_avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "num_trees = 100\n",
    "min_count = 10\n",
    "dataset = '1000k'\n",
    "path = '-'.join(['annoy','dim',str(embedding_dim),'tree',str(num_trees),'mincount',str(min_count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data file...')\n",
    "with open('data-'+dataset+'.msgpack','rb') as fp:\n",
    "    data = msgpack.load(fp,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tracks...\n",
      "Loading artists...\n",
      "Loading track to artist...\n"
     ]
    }
   ],
   "source": [
    "print('Loading tracks...')\n",
    "with open('tracks-'+dataset+'.pkl','rb') as fp:\n",
    "    track_uri2id,track_id2name = pickle.load(fp,encoding='utf-8')\n",
    "print('Loading artists...')\n",
    "with open('artists-'+dataset+'.pkl','rb') as fp:\n",
    "    artist_uri2id,artist_id2name = pickle.load(fp,encoding='utf-8')\n",
    "print('Loading track to artist...')\n",
    "with open('track2artist-'+dataset+'.pkl','rb') as fp:\n",
    "    track2artist = pickle.load(fp)\n",
    "track_id2uri = {v:k for k,v in track_uri2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec Model...\n",
      "Loaded Word2Vec Model...\n"
     ]
    }
   ],
   "source": [
    "w2v_model = getWord2Vec([],embedding_dim,min_count,path=path,precomputed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "artist_vecs = defaultdict(list)\n",
    "for k,v in track2artist.items():\n",
    "    if(str(k) in w2v_model.wv.vocab):\n",
    "        artist_vecs[v].append(w2v_model.wv[str(k)])\n",
    "for k in artist_vecs.keys():\n",
    "    artist_vecs[k] = np.mean(np.asarray(artist_vecs[k]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_vectors = averagePlaylistVec(data)\n",
    "#playlist_vectors = np.asarray(playlist_vectors)\n",
    "#playlist_vectors = list(np.delete(playlist_vectors,empty_playlists))\n",
    "with open(path+'/playlist_embeddings.pkl','wb') as fp:\n",
    "    pickle.dump(playlist_vectors,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/playlist_embeddings.pkl','rb') as fp:\n",
    "    playlist_vectors = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(playlist_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./annoy/annoy-250.ann\n"
     ]
    }
   ],
   "source": [
    "annoy_path = './annoy'\n",
    "num_annoy_trees = 250\n",
    "#annoy_indexer = createAnnoyIndex(playlist_vectors,num_annoy_trees,annoy_path)\n",
    "annoy_indexer = loadAnnoyIndex(embedding_dim,num_annoy_trees,annoy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del playlist_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe3a8f866cb41c6801f6b61e6f43081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "no_track_index = []\n",
    "one_track_index = []\n",
    "rest_index = []\n",
    "challenge_data = []\n",
    "challenge_pids = []\n",
    "names = []\n",
    "filename = '../../challenge_set.json'\n",
    "\n",
    "with open(filename, \"r\") as fp:\n",
    "    contents = json.load(fp)[\"playlists\"]\n",
    "    for idx,play_list in tqdm_notebook(enumerate(contents)):\n",
    "        song_ids = []\n",
    "        pid = play_list['pid']\n",
    "        challenge_pids.append(pid)\n",
    "        pname = ''\n",
    "        if('name' in play_list):\n",
    "            pname = play_list['name']\n",
    "        names.append(pname)\n",
    "        if(play_list[\"num_samples\"] == 0):\n",
    "            no_track_index.append(idx)\n",
    "        elif(play_list[\"num_samples\"] == 1):\n",
    "            song = play_list['tracks'][0]\n",
    "            if(song['track_uri'] in track_uri2id):\n",
    "                song_ids.append(str(track_uri2id[song['track_uri']]))\n",
    "                one_track_index.append(idx)\n",
    "            else:\n",
    "                no_track_index.append(idx)\n",
    "        else:\n",
    "            for song in play_list[\"tracks\"]:\n",
    "                if(song['track_uri'] in track_uri2id):\n",
    "                    song_ids.append(str(track_uri2id[song['track_uri']]))\n",
    "            if(len(song_ids)==0):\n",
    "                no_track_index.append(idx)\n",
    "            rest_index.append(idx)\n",
    "        challenge_data.append(song_ids)\n",
    "with open('../../challenge_data.pkl','wb') as fp:\n",
    "    pickle.dump(challenge_data,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = testPlaylistAvg(challenge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors[4344] = artist_vecs[50644]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/test_embeddings.pkl','wb') as fp:\n",
    "    pickle.dump(test_vectors,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/test_embeddings.pkl','rb') as fp:\n",
    "    test_vectors = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(annoy_indexer.get_nns_by_item(74848, 5,include_distances=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topN(vector,inputs,top=500):\n",
    "    num_already_in_playlist = len(inputs)\n",
    "    num_required = top+num_already_in_playlist\n",
    "    num_similar_playlists = 600\n",
    "    similar_playlists = annoy_indexer.get_nns_by_vector(vector,num_similar_playlists,include_distances=True)\n",
    "    song_count = dict()\n",
    "    for idx in range(num_similar_playlists):\n",
    "        distance = similar_playlists[1][idx]\n",
    "        playlist_score = distance*5000*math.exp(-0.22*idx)\n",
    "        playlist_id = similar_playlists[0][idx]\n",
    "        playlist = data[playlist_id]\n",
    "        for songid in playlist:\n",
    "            if songid in song_count:\n",
    "                song_count[songid] = (song_count[songid][0] + playlist_score, song_count[songid][1] + 1)\n",
    "            else:\n",
    "                song_count[songid] = (playlist_score,1)\n",
    "        if(len(song_count)>=1.5*num_required):\n",
    "            break\n",
    "    approx_list = []\n",
    "    topN = sorted(song_count.items(), key=lambda x: x[1][0]/x[1][1], reverse=True)[:num_required]\n",
    "    for entry in topN:\n",
    "        songid = entry[0]\n",
    "        if(songid in inputs):\n",
    "            continue\n",
    "        #title_and_artist = ' '.join([track_id2name[int(songid)],'-',artist_id2name[track2artist[int(songid)]]])\n",
    "        #approx_list.append(title_and_artist)\n",
    "        approx_list.append(track_id2uri[int(songid)])\n",
    "    \n",
    "    return approx_list[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ncae/output_embeddings.pkl','rb') as fp:\n",
    "    output_vectors = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d183eb9725de4e9996a7f2f6230b912f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations = []\n",
    "recommendations.append(['team_info','main','neural-panda','v18saboo@g.ucla.edu'])\n",
    "for idx,playlist in tqdm_notebook(enumerate(test_vectors)):\n",
    "    pid = challenge_pids[idx]\n",
    "    rec = get_topN(playlist,challenge_data[idx])\n",
    "    recommendations.append([pid] + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(recommendations)):\n",
    "    if len(recommendations[i]) < 501:\n",
    "        print('Length Error at',i,', Expected 500, Got',len(recommendations[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topN(vector,inputs,n=500):\n",
    "    num_already_in_playlist = len(inputs)\n",
    "    approximate_neighbors = w2v_model.wv.most_similar([vector], topn=n+num_already_in_playlist)\n",
    "    approx_list = []\n",
    "    approx_artist = set()\n",
    "    for pair in approximate_neighbors:\n",
    "        songid = pair[0]\n",
    "        if(songid in inputs):\n",
    "            continue\n",
    "        title_and_artist = ' '.join([track_id2name[int(songid)],'-',artist_id2name[track2artist[int(songid)]]])\n",
    "        approx_list.append(title_and_artist)\n",
    "        #approx_list.append(track_id2uri[int(songid)])\n",
    "        #approx_artist.add(artist_id2name[track2artist[songid]])\n",
    "    \n",
    "    return approx_list[:10]\n",
    "\n",
    "rec = get_topN(output_vectors[0],challenge_data[0])\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = pickle.load(open('annoy-dim-256-tree-100-mincount-10/playlist_names.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s:\n",
    "    print(choices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = get_topN(test_vectors[-1],challenge_data[-1],500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_indexer.get_n_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices[257168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations[-1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(recommendations[-1][1:]).intersection(set(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in data:\n",
    "    l.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 60.308631\n",
      "Median = 44.0\n",
      "Mode = 20\n",
      "Maximum = 250\n",
      "3rd Quartile = 83.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean =',np.mean(l))\n",
    "print('Median =',np.median(l))\n",
    "counts = np.bincount(l)\n",
    "print('Mode =',np.argmax(counts))\n",
    "print('Maximum =',np.max(l))\n",
    "print('3rd Quartile =',np.percentile(l,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
