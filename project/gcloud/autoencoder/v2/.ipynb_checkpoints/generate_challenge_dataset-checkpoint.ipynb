{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord2Vec(data,embedding_dim,min_count=5,path='',num_trees=100,precomputed=False):\n",
    "    directory = path\n",
    "    annoy_fname = directory + '/annoy'\n",
    "    w2v_fname = directory + '/word2vec-' + str(embedding_dim) + '.txt'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    if(not precomputed):\n",
    "        print('Creating Word2Vec Model...')\n",
    "        model = Word2Vec(data, size=embedding_dim, window=5, min_count=min_count, workers=6, iter=10,negative=10)\n",
    "        model.init_sims()\n",
    "        print('Initialized Sims')\n",
    "        model.save(w2v_fname)\n",
    "        print('Creating Annoy Indexes...')\n",
    "        annoy_index = AnnoyIndexer(model, num_trees)\n",
    "        # Persist index to disk\n",
    "        annoy_index.save(annoy_fname)\n",
    "    else:\n",
    "        print('Loading Word2Vec Model...')\n",
    "        model = Word2Vec.load(w2v_fname)\n",
    "        print('Loading Annoy Indexing...')\n",
    "        annoy_index = AnnoyIndexer()\n",
    "        annoy_index.load(annoy_fname)\n",
    "        annoy_index.model = model\n",
    "        print('Done Loading')\n",
    "\n",
    "    return model,annoy_index#,embedding_matrix\n",
    "\n",
    "def averagePlaylistVec(data):\n",
    "    avg_vec = np.zeros_like(w2v_model.wv.vectors[0])\n",
    "    playlist_vectors = []\n",
    "    empty_playlists = []\n",
    "    pid_order = []\n",
    "    for index,playlist in enumerate(tqdm_notebook(data)):\n",
    "        w2vecs = []\n",
    "        for songid in playlist:\n",
    "            if(songid in w2v_model.wv.vocab):\n",
    "                vec = w2v_model.wv[songid]\n",
    "                w2vecs.append(vec)\n",
    "        if(len(w2vecs)==0):\n",
    "            empty_playlists.append(index)\n",
    "            continue\n",
    "        pid_order.append(index)\n",
    "        avg_vec = np.mean(np.asarray(w2vecs),axis=0)\n",
    "        playlist_vectors.append(avg_vec)\n",
    "    return playlist_vectors,pid_order,empty_playlists\n",
    "\n",
    "def get_topN(vector,inputs,n=40):\n",
    "    num_already_in_playlist = len(inputs)\n",
    "    approximate_neighbors = w2v_model.wv.most_similar([vector], topn=n+num_already_in_playlist, indexer=annoy_index)\n",
    "    approx_list = []\n",
    "    approx_artist = set()\n",
    "    for pair in approximate_neighbors:\n",
    "        songid = pair[0]\n",
    "        if(songid in inputs):\n",
    "            continue\n",
    "        approx_list.append(songid)\n",
    "    return approx_list[:n]\n",
    "\n",
    "\n",
    "def precision_at_k(expected, predicted):\n",
    "    \"\"\"\n",
    "    Compute precision@k metric. Also known as hit-rate.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predicted : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for i in range(len(expected)):\n",
    "        #predicted[i] = predicted[i][:len(expected[i])]\n",
    "        precision = float(len(set(predicted[i]) & set(expected[i]))) / float(len(predicted[i]))\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions) \n",
    "    \n",
    "    \n",
    "def compute_dcg(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute DCG score for each user.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list\n",
    "        Ground truth recommendations for single playlist.\n",
    "    predicted : list\n",
    "        Predicted recommendations for single playlist.\n",
    "    \"\"\"\n",
    "    score = [float(el in expected) for el in predicted]\n",
    "    dcg = np.sum(score / np.log2(1 + np.arange(1, len(score) + 1)))\n",
    "    return dcg\n",
    "\n",
    "def dcg_at_k(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute dcg@k metric. (Discounted Continuous Gain)\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predictions : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    dcg_scores = []\n",
    "    \n",
    "    for i in range(len(expected)):\n",
    "        #predicted[i] = predicted[i][:len(expected[i])]\n",
    "        dcg = compute_dcg(expected[i],predicted[i])\n",
    "        dcg_scores.append(dcg)\n",
    "    return np.mean(dcg_scores)\n",
    "\n",
    "def ndcg_at_k(expected,predicted):\n",
    "    \"\"\"\n",
    "    Compute ndcg@k metric. (Normalized Discounted Continous Gain)\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    expected : list of list\n",
    "        Ground truth recommendations for each playlist.\n",
    "    predicted : list of list\n",
    "        Predicted recommendations for each playlist.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    for i in range(len(expected)):\n",
    "        #predicted[i] = predicted[i][:len(expected[i])]\n",
    "        labels = expected[i]\n",
    "        idcg = compute_dcg(labels,labels)\n",
    "        true_dcg = compute_dcg(labels,predicted[i])\n",
    "        ndcg_scores.append(true_dcg/idcg)\n",
    "    return np.mean(ndcg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file...\n",
      "Done Loading!\n"
     ]
    }
   ],
   "source": [
    "thresh = 80\n",
    "leave_out = 20\n",
    "dataset = 1000\n",
    "pid_list = []\n",
    "test_set = []\n",
    "train_set = []\n",
    "print('Loading data file...')\n",
    "with open('data-'+str(dataset)+'k.msgpack','rb') as fp:\n",
    "    data = msgpack.load(fp,encoding='utf-8')\n",
    "print('Done Loading!')\n",
    "counter = 0    \n",
    "for i,d in enumerate(data):\n",
    "    if len(d) > thresh:\n",
    "        pid_list.append(i)\n",
    "        test_set.append(d[-1*leave_out:])\n",
    "        train_set.append(d[:len(d)-leave_out])\n",
    "        counter += 1\n",
    "        if counter > 9999:\n",
    "            break\n",
    "        \n",
    "with open('my_chlng_train.msgpack', 'wb') as fp:\n",
    "    msgpack.dump(train_set, fp)\n",
    "with open('my_chlng_test.msgpack', 'wb') as fp:\n",
    "    msgpack.dump(test_set, fp)\n",
    "with open('my_chlng_pid_list.msgpack', 'wb') as fp:\n",
    "    msgpack.dump(pid_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "num_trees = 100\n",
    "min_count = 10\n",
    "path = '-'.join(['annoy','dim',str(embedding_dim),'tree',str(num_trees),'mincount',str(min_count)])\n",
    "w2v_model,annoy_index = getWord2Vec([],embedding_dim,min_count,path=path,precomputed=True)\n",
    "playlist_vectors, _, empty_playlists = averagePlaylistVec(train_set)\n",
    "train_set = np.asarray(train_set)\n",
    "train_set = list(np.delete(train_set,empty_playlists))\n",
    "test_set = np.asarray(test_set)\n",
    "test_set = list(np.delete(test_set,empty_playlists))\n",
    "pid_list = np.asarray(pid_list)\n",
    "pid_list = list(np.delete(pid_list,empty_playlists))\n",
    "with open('my_chlng_train.msgpack', 'wb') as fp:\n",
    "    msgpack.dump(train_set, fp)\n",
    "with open('my_chlng_test.msgpack', 'wb') as fp:\n",
    "    msgpack.dump(test_set, fp)\n",
    "with open('my_chlng_pid_list.pkl', 'wb') as fp:\n",
    "    pickle.dump(pid_list, fp)\n",
    "with open('my_chlng_plist_embed.pkl', 'wb') as fp:\n",
    "    pickle.dump(playlist_vectors, fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ncae/challenge_output_embeddings.pkl','rb') as fp:\n",
    "    output_vectors = pickle.load(fp)\n",
    "with open('my_chlng_train.msgpack','rb') as fp:\n",
    "    train_set = msgpack.load(fp, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f033c1f16a3b4fb5a947dc461882a1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_preds = []\n",
    "for output,inputs in tqdm_notebook(zip(output_vectors, train_set)):\n",
    "    output_preds.append(get_topN(output, inputs, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4044ed12cdb4fd08d0f2fa1b38a83e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hit rate: 0.33224325444463504\n",
      "NDCG: 0.42361688820673576\n",
      "DCG: 1.222424066823299\n"
     ]
    }
   ],
   "source": [
    "with open('my_chlng_test.msgpack','rb') as fp:\n",
    "    test_set = msgpack.load(fp, encoding='utf-8')\n",
    "hr = []\n",
    "ndcg = []\n",
    "dcg = []\n",
    "for op,ip in tqdm_notebook(zip(output_preds, test_set)):\n",
    "    hr.append(precision_at_k(ip, op[:20]))\n",
    "    ndcg.append(ndcg_at_k(ip, op[:20]))\n",
    "    dcg.append(dcg_at_k(ip, op[:20]))\n",
    "print('Hit rate:', np.mean(np.asarray(hr)))\n",
    "print('NDCG:', np.mean(np.asarray(ndcg)))\n",
    "print('DCG:',np.mean(np.asarray(dcg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee755c5864f450daefca0aa302ec741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hit rate: 0.0019262705082032816\n",
      "NDCG: 0.01031328324882872\n",
      "DCG: 0.07260828197055023\n"
     ]
    }
   ],
   "source": [
    "with open('my_chlng_test.msgpack','rb') as fp:\n",
    "    test_set = msgpack.load(fp, encoding='utf-8')\n",
    "hr = []\n",
    "ndcg = []\n",
    "dcg = []\n",
    "for op,ip in tqdm_notebook(zip(output_preds, test_set)):\n",
    "    ip = list(map(int, ip))\n",
    "    op = list(map(int, op))\n",
    "    hr.append(precision_at_k([ip], [op]))\n",
    "    ndcg.append(ndcg_at_k([ip], [op]))\n",
    "    dcg.append(dcg_at_k([ip], [op]))\n",
    "print('Hit rate:', np.mean(np.asarray(hr)))\n",
    "print('NDCG:', np.mean(np.asarray(ndcg)))\n",
    "print('DCG:',np.mean(np.asarray(dcg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
